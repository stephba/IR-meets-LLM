{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# TODO: maybe load the dataset as follows from huggingface\n",
    "# https://huggingface.co/docs/datasets/v1.9.0/loading_datasets.html\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup generative model (open/closed model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "\n",
    "class GenerativeModel:\n",
    "    def __init__(self, max_answer_length) -> None:\n",
    "        self.generator = pipeline('text-generation', model=\"facebook/opt-1.3b\")\n",
    "        self.model = OPTForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "        self.tokenizer.padding_side = \"left\"   # so that the text will continue as without padding\n",
    "        self.max_answer_length = max_answer_length\n",
    "\n",
    "    def get_open_model_answer(self, question, context, use_pipeline=False):\n",
    "        prompt = f\"CONTEXT:\\n{context}\\nQUESTION:\\n{question}\"\n",
    "        # generate answer\n",
    "        answer = self._generate_answer(prompt, use_pipeline)\n",
    "        # remove prompt from generated text\n",
    "        answer = answer.removeprefix(prompt)\n",
    "        return answer\n",
    "\n",
    "    def get_closed_model_answer(self, question, use_pipeline=False):\n",
    "        prompt = question\n",
    "        # generate answer\n",
    "        answer = self._generate_answer(prompt, use_pipeline)\n",
    "        # remove prompt from generated text\n",
    "        answer = answer.removeprefix(prompt)\n",
    "        return answer\n",
    "\n",
    "    def get_open_batch_answers(self, questions, contexts):\n",
    "        assert len(questions) == len(contexts), \"questions and contexts should have the same length\"\n",
    "        prompts = [f\"CONTEXT:\\n{contexts[i]}\\nQUESTION:\\n{questions[i]}\" for i in range(len(questions))]\n",
    "        # generate answers\n",
    "        answers = self._generate_batch_answers(prompts)\n",
    "        # remove prompts from generated text\n",
    "        answers = [answers[i].removeprefix(prompts[i]) for i in range(len(prompts))]\n",
    "        return answers\n",
    "\n",
    "    # https://github.com/huggingface/transformers/issues/10704\n",
    "    def get_closed_batch_answers(self, questions):\n",
    "        prompts = questions\n",
    "        # generate answers\n",
    "        answers = self._generate_batch_answers(prompts)\n",
    "        # remove prompts from generated text\n",
    "        # TEMP answers = [answers[i].removeprefix(prompts[i]) for i in range(len(prompts))]\n",
    "        return answers\n",
    "    \n",
    "    def _generate_answer(self, prompt, use_pipeline):\n",
    "        if use_pipeline:\n",
    "            answer = self.generator(prompt, max_new_tokens=self.max_answer_length)[0]['generated_text']\n",
    "        else:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "            generate_ids = self.model.generate(inputs.input_ids, max_new_tokens=self.max_answer_length)\n",
    "            answer = self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        return answer\n",
    "\n",
    "    def _generate_batch_answers(self, prompts):\n",
    "        inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True)  # padding, so that all prompts have same length for computing it as a batch\n",
    "        generate_ids = self.model.generate(inputs.input_ids, max_new_tokens=self.max_answer_length)\n",
    "        answers = self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = GenerativeModel(max_answer_length=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary test of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/stephan/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: test it with some squad data\n",
    "from datasets import load_dataset\n",
    "test_set = load_dataset('squad', split='validation[:10]')  # get first n entries from the test set\n",
    "test_contexts = test_set['context']  # list of strings\n",
    "test_questions = test_set['question']  # list of strings\n",
    "test_answers = [d['text'] for d in test_set['answers']]  # list of lists of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for single question\n",
    "print(f\"Context: {test_contexts[0]}\\nQuestion: {test_questions[0]}\\nCorrect answer: {test_answers[0]}\")\n",
    "print(\"Closed generative Model:\")\n",
    "print(generative_model.get_closed_model_answer(test_questions[0]))\n",
    "print(\"Open generative Model:\")\n",
    "print(generative_model.get_open_model_answer(test_questions[0], test_contexts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test for batch questioning closed model\n",
    "closed_answers = generative_model.get_closed_batch_answers(test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# test for batch questioning open model\n",
    "open_answers = generative_model.get_open_batch_answers(test_questions, test_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
