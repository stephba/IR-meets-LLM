{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# TODO: maybe load the dataset as follows from huggingface\n",
    "# https://huggingface.co/docs/datasets/v1.9.0/loading_datasets.html\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup generative model (open/closed model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "\n",
    "class GenerativeModel:\n",
    "    def __init__(self, max_answer_length) -> None:\n",
    "        self.generator = pipeline('text-generation', model=\"facebook/opt-1.3b\")\n",
    "        self.model = OPTForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "        self.max_answer_length = max_answer_length\n",
    "\n",
    "    def get_open_model_answer(self, question, context, use_pipeline=False):\n",
    "        prompt = f\"CONTEXT:\\n{context}\\nQUESTION:\\n{question}\"\n",
    "        # generate answer\n",
    "        answer = self._generate_answer(prompt, use_pipeline)\n",
    "        # remove prompt from generated text\n",
    "        answer = answer.removeprefix(prompt)\n",
    "        return answer\n",
    "\n",
    "    def get_closed_model_answer(self, question, use_pipeline=False):\n",
    "        prompt = question\n",
    "        # generate answer\n",
    "        answer = self._generate_answer(prompt, use_pipeline)\n",
    "        # remove prompt from generated text\n",
    "        answer = answer.removeprefix(prompt)\n",
    "        return answer\n",
    "\n",
    "    def get_open_model_answers_batched(self, questions, contexts):\n",
    "        # COMBAK\n",
    "        pass\n",
    "\n",
    "    def get_closed_model_answer(self, questions):\n",
    "        # COMBAK\n",
    "        pass\n",
    "    \n",
    "    def _generate_answer(self, prompt, use_pipeline):\n",
    "        if use_pipeline:\n",
    "            answer = self.generator(prompt, max_new_tokens=self.max_answer_length)[0]['generated_text']\n",
    "        else:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "            generate_ids = self.model.generate(inputs.input_ids, max_new_tokens=self.max_answer_length)\n",
    "            answer = self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = GenerativeModel(max_answer_length=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary test of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: test it with some squad data\n",
    "from datasets import load_dataset\n",
    "test_set = load_dataset('squad', split='validation[:10]')  # get first 10 entries from the test set\n",
    "test_contexts = test_set['context']  # list of strings\n",
    "test_questions = test_set['question']  # list of strings\n",
    "test_answers = [d['text'] for d in test_set['answers']]  # list of lists of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for single question\n",
    "print(f\"Context: {test_contexts[0]}\\nQuestion: {test_questions[0]}\\nCorrect answer: {test_answers[0]}\")\n",
    "print(\"Closed generative Model:\")\n",
    "print(generative_model.get_closed_model_answer(test_questions[0]))\n",
    "print(\"Open generative Model:\")\n",
    "print(generative_model.get_open_model_answer(test_questions[0], test_contexts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
